{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rQ6G65n6OxV"
   },
   "source": [
    "# Form 13\n",
    "This notebook describes how to use Neo4j and SageMaker together.  In it you connect to a Neo4j instance, load data and compute an embedding.  You then load that data into AWS S3.  Finally, you use SageMaker to train a model using the new embedding as an additional feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdMFRbqGzSqF"
   },
   "source": [
    "## Deploy Neo4j\n",
    "You're going to need a Neo4j deployment to run this lab.  The easiest way to get that is via the [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=23ec694a-d2af-4641-b4d3-b7201ab2f5f9).  Select \"Neo4j Enterprise Edition\" and deploy that.  Suggested parameters are:\n",
    "\n",
    "* Graph Database Version - 4.4.8\n",
    "* Install Graph Data Science - True\n",
    "* Graph Data Science License Key - None\n",
    "* Install Bloom - True\n",
    "* Bloom License Key - None\n",
    "* Password - Enter something here\n",
    "* Node Count - 1\n",
    "* Instance Type - t3.xlarge\n",
    "* Key Name - Pick a key you have\n",
    "* SSH CIDR - 0.0.0.0/0\n",
    "\n",
    "The Marketplace listing deploys an ASG.  When deployment is complete, you can get the IP address of your Neo4j node from that ASG.  You can view deployed ASGs [here](https://us-east-1.console.aws.amazon.com/ec2autoscaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MwTYwKk6OxX"
   },
   "source": [
    "## Using the Neo4j API\n",
    "Now that we have a Neo4j deployment, let's connect to Neo4j.  First off, install the Neo4j Graph Data Science package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT0KaLYj6OxX"
   },
   "outputs": [],
   "source": [
    "%pip install graphdatascience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFokFbiL6OxY"
   },
   "source": [
    "Now, you're going to need the connection string and credentials from the deployment you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P41l_P4zzSqF"
   },
   "outputs": [],
   "source": [
    "# Edit these variables!\n",
    "DB_URL = 'neo4j://ec2-3-91-14-235.compute-1.amazonaws.com:7687'\n",
    "DB_PASS = 'foo123'\n",
    "\n",
    "# You can leave this default\n",
    "DB_USER = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8lUkSvmozSqF"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(DB_URL, auth=(DB_USER, DB_PASS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7-MlyTU6OxZ"
   },
   "source": [
    "## Load Data into Neo4j\n",
    "Now that we've got our connection object, let's load the dataset into Neo4j.\n",
    "\n",
    "The dataset is pulled from the SEC's EDGAR database. These are public filings of something called Form 13. Asset managers with over \\$100m AUM are required to submit Form 13 quarterly. That's then made available to the public over http. The csvs linked above were pulled from EDGAR using some python scripts. If you're curious, they're all available [here](https://github.com/neo4j-partners/neo4j-sec-edgar-form13). We've filtered the data to only include filings over \\$10m in value.\n",
    "\n",
    "We're going to create constratints for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxgUxjVQ6OxZ"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher('CREATE CONSTRAINT IF NOT EXISTS ON (p:Company) ASSERT (p.cusip) IS NODE KEY;')\n",
    "display(result)\n",
    "\n",
    "result = gds.run_cypher('CREATE CONSTRAINT IF NOT EXISTS ON (p:Manager) ASSERT (p.filingManager) IS NODE KEY;')\n",
    "display(result)\n",
    "\n",
    "result = gds.run_cypher('CREATE CONSTRAINT IF NOT EXISTS ON (p:Holding) ASSERT (p.filingManager, p.cusip, p.reportCalendarOrQuarter) IS NODE KEY;')\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdKOItse6Oxa"
   },
   "source": [
    "Now let's load the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgCgdkCt6Oxa"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/2021.csv' AS row\n",
    "        MERGE (c:Company {cusip:row.cusip})\n",
    "        ON CREATE SET\n",
    "            c.nameOfIssuer=row.nameOfIssuer\n",
    "    \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqJZYNES6Oxa"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/2021.csv' AS row\n",
    "        MERGE (m:Manager {filingManager:row.filingManager})\n",
    "    \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rERDJtCi6Oxa"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/2021.csv' AS row\n",
    "        MERGE (h:Holding {filingManager:row.filingManager, cusip:row.cusip, reportCalendarOrQuarter:row.reportCalendarOrQuarter})\n",
    "        ON CREATE SET\n",
    "            h.value=row.value, \n",
    "            h.shares=row.shares,\n",
    "            h.target=row.target,\n",
    "            h.nameOfIssuer=row.nameOfIssuer\n",
    "    \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzdC3x316Oxa"
   },
   "source": [
    "Now let's create relationships between those nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rggD5Yho6Oxa"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/2021.csv' AS row\n",
    "        MATCH (m:Manager {filingManager:row.filingManager})\n",
    "        MATCH (h:Holding {filingManager:row.filingManager, cusip:row.cusip, reportCalendarOrQuarter:row.reportCalendarOrQuarter})\n",
    "        MERGE (m)-[r:OWNS]->(h)\n",
    "    \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpsRbdhe6Oxb"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/2021.csv' AS row\n",
    "        MATCH (h:Holding {filingManager:row.filingManager, cusip:row.cusip, reportCalendarOrQuarter:row.reportCalendarOrQuarter})\n",
    "        MATCH (c:Company {cusip:row.cusip})\n",
    "        MERGE (h)-[r:PARTOF]->(c)\n",
    "    \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtJy4eO_zSqF"
   },
   "source": [
    "## Graph Data Science\n",
    "Now we're going to use Neo4j Graph Data Science to create an in memory graph represtation of the data.  We'll enhance that represation with features we engineer using a graph embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x76ZEtR16Oxb"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.create(\n",
    "      'mygraph',\n",
    "      ['Company', 'Manager', 'Holding'],\n",
    "      {\n",
    "          OWNS: {orientation: 'UNDIRECTED'},\n",
    "          PARTOF: {orientation: 'UNDIRECTED'}\n",
    "      }\n",
    "    )\n",
    "    YIELD\n",
    "      graphName AS graph,\n",
    "      relationshipProjection AS readProjection,\n",
    "      nodeCount AS nodes,\n",
    "      relationshipCount AS rels\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiwL552u6Oxb"
   },
   "source": [
    "If you get an error saying the graph already exists, that's probably because you ran this code before. You can destroy it using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPZIIIJc6Oxb"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.drop('mygraph')\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG1novOj6Oxb"
   },
   "source": [
    "Now, let's list the details of the graph to make sure the projection was created as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyaw5itE6Oxb"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.list()\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEQAChAa6Oxb"
   },
   "source": [
    "Now we can generate an embedding from that graph. This is a new feature we can use in our predictions. We're using FastRP, which is a more full featured and higher performance of Node2Vec. You can learn more about that [here](https://neo4j.com/docs/graph-data-science/current/algorithms/fastrp/).\n",
    "\n",
    "There are a bunch of parameters we could adjust in this.  One of the most obvious is the embeddingDimension.  The documentation covers many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLFxuPb66Oxc"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "  CALL gds.fastRP.mutate('mygraph',{\n",
    "    embeddingDimension: 16,\n",
    "    randomSeed: 1,\n",
    "    mutateProperty:'embedding'\n",
    "  })\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRpgM-NV6Oxc"
   },
   "source": [
    "That creates an embedding for each node type.  However, we only want the embedding on the nodes of type holding.\n",
    "\n",
    "We're going to take the embedding from our projection and write it to the holding nodes in the underlying database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dBS16zD6Oxc"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    CALL gds.graph.writeNodeProperties('mygraph', ['embedding'], ['Holding'])\n",
    "    YIELD writeMillis\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK6LeBne6Oxc"
   },
   "outputs": [],
   "source": [
    "result = gds.run_cypher(\n",
    "  \"\"\"\n",
    "    MATCH (n:Holding) RETURN n\n",
    "  \"\"\"\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N_x38Ci6Oxc"
   },
   "source": [
    "Note that this query will take 2-3 minutes to run as it's grabbing nearly half a million nodes along with all their properties and our new embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "197ZaAH16Oxc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([dict(record.items()) for record in result['n']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3esUO8s6Oxc"
   },
   "source": [
    "Note that the embedding row is an array. To make this dataset more consumable, we should flatten that out into multiple individual features: embedding_0, embedding_1, ... embedding_n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i0_txCB6Oxc"
   },
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(df['embedding'].values.tolist()).add_prefix(\"embedding_\")\n",
    "merged = df.drop(columns=['embedding']).merge(embeddings, left_index=True, right_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zb7lH366Oxc"
   },
   "source": [
    "Now that we have the data formatted properly, let's split it into training, testing and validation sets.  We'll write those to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uLg34zlu6Oxc"
   },
   "outputs": [],
   "source": [
    "df = merged\n",
    "\n",
    "train = df.loc[df['reportCalendarOrQuarter'] == '03-31-2021']\n",
    "train.to_csv('train.csv', index=False)\n",
    "\n",
    "test = df.loc[df['reportCalendarOrQuarter'] == '06-30-2021']\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "validate = df.loc[df['reportCalendarOrQuarter'] == '09-30-2021']\n",
    "validate.to_csv('validate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Connection\n",
    "Let's setup our SageMaker connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/form13'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3\n",
    "Now we're going to upload the training and testing data to our default SageMaker bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to: s3://sagemaker-us-east-1-159878781974/sagemaker/form13/train/train.csv\n",
      "Testing data uploaded to: s3://sagemaker-us-east-1-159878781974/sagemaker/form13/test/test.csv\n",
      "Validation data uploaded to: s3://sagemaker-us-east-1-159878781974/sagemaker/form13/validate/validate.csv\n"
     ]
    }
   ],
   "source": [
    "train_data_s3_path = session.upload_data(path='train.csv', key_prefix=prefix + '/train')\n",
    "print('Training data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_data_s3_path = session.upload_data(path='test.csv', key_prefix=prefix + '/test')\n",
    "print('Testing data uploaded to: ' + test_data_s3_path)\n",
    "\n",
    "validation_data_s3_path = session.upload_data(path='validate.csv', key_prefix=prefix + '/validate')\n",
    "print('Validation data uploaded to: ' + validation_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the SageMaker Autopilot Job\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml_job_config = {'CompletionCriteria': {'MaxCandidates': 3}}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': 's3://{}/{}/train'.format(bucket, prefix),\n",
    "            }\n",
    "        },\n",
    "        'TargetAttributeName': 'target',\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {'S3OutputPath': 's3://{}/{}/output'.format(bucket, prefix)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the SageMaker Autopilot Job\n",
    "You can now launch the Autopilot job by calling the create_auto_ml_job method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoMLJobName: automl-form13-22-17-26-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AutoMLJobArn': 'arn:aws:sagemaker:us-east-1:159878781974:automl-job/automl-form13-22-17-26-11',\n",
       " 'ResponseMetadata': {'RequestId': '7f35a65f-6e71-4f90-9997-bba05bfe76c1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7f35a65f-6e71-4f90-9997-bba05bfe76c1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '96',\n",
       "   'date': 'Wed, 22 Jun 2022 17:26:12 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "auto_ml_job_name = 'automl-form13-' + timestamp_suffix\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=auto_ml_job_config,\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking SageMaker Autopilot job progress\n",
    "SageMaker Autopilot job consists of the following high-level steps : * Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets. * Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level. * Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobStatus - Secondary Status\n",
      "------------------------------\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingExplainabilityReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "InProgress - GeneratingModelInsightsReport\n",
      "Completed - Completed\n"
     ]
    }
   ],
   "source": [
    "print('JobStatus - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print(describe_response['AutoMLJobStatus'] + ' - ' + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "    print(\n",
    "        describe_response['AutoMLJobStatus'] + ' - ' + describe_response['AutoMLJobSecondaryStatus']\n",
    "    )\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now use the describe_auto_ml_job API to look up the best candidate selected by the SageMaker Autopilot job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateName: automl-form13-22-17-26-11smta1hA-001-ae93f26b\n",
      "FinalAutoMLJobObjectiveMetricName: validation:f1_binary\n",
      "FinalAutoMLJobObjectiveMetricValue: 0.5180100202560425\n",
      "\n",
      "{'CandidateName': 'automl-form13-22-17-26-11smta1hA-001-ae93f26b',\n",
      " 'CandidateProperties': {'CandidateArtifactLocations': {'Explainability': 's3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-17-26-11/documentation/explainability/output',\n",
      "                                                        'ModelInsights': 's3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-17-26-11/documentation/model_monitor/output'},\n",
      "                         'CandidateMetrics': [{'MetricName': 'F1',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'F1',\n",
      "                                               'Value': 0.5180100202560425},\n",
      "                                              {'MetricName': 'LogLoss',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'LogLoss',\n",
      "                                               'Value': 0.5751699805259705},\n",
      "                                              {'MetricName': 'Recall',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'Recall',\n",
      "                                               'Value': 0.4165000021457672},\n",
      "                                              {'MetricName': 'Precision',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'Precision',\n",
      "                                               'Value': 0.684939980506897},\n",
      "                                              {'MetricName': 'AUC',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'AUC',\n",
      "                                               'Value': 0.7465999722480774},\n",
      "                                              {'MetricName': 'Accuracy',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'Accuracy',\n",
      "                                               'Value': 0.6936399936676025},\n",
      "                                              {'MetricName': 'BalancedAccuracy',\n",
      "                                               'Set': 'Validation',\n",
      "                                               'StandardMetricName': 'BalancedAccuracy',\n",
      "                                               'Value': 0.6456400156021118}]},\n",
      " 'CandidateStatus': 'Completed',\n",
      " 'CandidateSteps': [{'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:159878781974:processing-job/automl-form13-22-17-26-11-db-1-694f76e65daf4da3919287a57ed883fc',\n",
      "                     'CandidateStepName': 'automl-form13-22-17-26-11-db-1-694f76e65daf4da3919287a57ed883fc',\n",
      "                     'CandidateStepType': 'AWS::SageMaker::ProcessingJob'},\n",
      "                    {'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:159878781974:training-job/automl-form13-22-17-26-11-dpp1-1-dc69633130a24a0cb6cc879a480837',\n",
      "                     'CandidateStepName': 'automl-form13-22-17-26-11-dpp1-1-dc69633130a24a0cb6cc879a480837',\n",
      "                     'CandidateStepType': 'AWS::SageMaker::TrainingJob'},\n",
      "                    {'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:159878781974:transform-job/automl-form13-22-17-26-11-dpp1-csv-1-0ed9a1872abd449194e232b14a',\n",
      "                     'CandidateStepName': 'automl-form13-22-17-26-11-dpp1-csv-1-0ed9a1872abd449194e232b14a',\n",
      "                     'CandidateStepType': 'AWS::SageMaker::TransformJob'},\n",
      "                    {'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:159878781974:training-job/automl-form13-22-17-26-11smta1ha-001-ae93f26b',\n",
      "                     'CandidateStepName': 'automl-form13-22-17-26-11smta1hA-001-ae93f26b',\n",
      "                     'CandidateStepType': 'AWS::SageMaker::TrainingJob'}],\n",
      " 'CreationTime': datetime.datetime(2022, 6, 22, 17, 52, 38, tzinfo=tzlocal()),\n",
      " 'EndTime': datetime.datetime(2022, 6, 22, 18, 11, 10, tzinfo=tzlocal()),\n",
      " 'FinalAutoMLJobObjectiveMetric': {'MetricName': 'validation:f1_binary',\n",
      "                                   'Value': 0.5180100202560425},\n",
      " 'InferenceContainers': [{'Environment': {'AUTOML_TRANSFORM_MODE': 'feature-transform',\n",
      "                                          'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'application/x-recordio-protobuf',\n",
      "                                          'SAGEMAKER_PROGRAM': 'sagemaker_serve',\n",
      "                                          'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'},\n",
      "                          'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3',\n",
      "                          'ModelDataUrl': 's3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-17-26-11/data-processor-models/automl-form13-22-17-26-11-dpp1-1-dc69633130a24a0cb6cc879a480837/output/model.tar.gz'},\n",
      "                         {'Environment': {'MAX_CONTENT_LENGTH': '20971520',\n",
      "                                          'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'text/csv',\n",
      "                                          'SAGEMAKER_INFERENCE_OUTPUT': 'predicted_label',\n",
      "                                          'SAGEMAKER_INFERENCE_SUPPORTED': 'predicted_label,probability,probabilities'},\n",
      "                          'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3',\n",
      "                          'ModelDataUrl': 's3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-17-26-11/tuning/automl-for-dpp1-xgb/automl-form13-22-17-26-11smta1hA-001-ae93f26b/output/model.tar.gz'},\n",
      "                         {'Environment': {'AUTOML_TRANSFORM_MODE': 'inverse-label-transform',\n",
      "                                          'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'text/csv',\n",
      "                                          'SAGEMAKER_INFERENCE_INPUT': 'predicted_label',\n",
      "                                          'SAGEMAKER_INFERENCE_OUTPUT': 'predicted_label',\n",
      "                                          'SAGEMAKER_INFERENCE_SUPPORTED': 'predicted_label,probability,labels,probabilities',\n",
      "                                          'SAGEMAKER_PROGRAM': 'sagemaker_serve',\n",
      "                                          'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'},\n",
      "                          'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3',\n",
      "                          'ModelDataUrl': 's3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-17-26-11/data-processor-models/automl-form13-22-17-26-11-dpp1-1-dc69633130a24a0cb6cc879a480837/output/model.tar.gz'}],\n",
      " 'LastModifiedTime': datetime.datetime(2022, 6, 22, 18, 14, 43, 335000, tzinfo=tzlocal()),\n",
      " 'ObjectiveStatus': 'Succeeded'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "\n",
    "print('CandidateName: ' + best_candidate_name)\n",
    "print('FinalAutoMLJobObjectiveMetricName: ' + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print('FinalAutoMLJobObjectiveMetricValue: ' + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n",
    "print()\n",
    "pprint.pprint(best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference\n",
    "Now that we completed the SageMaker Autopilot job on the dataset, let's create a model from the best candidatewith Inference Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ARN corresponding to the best candidate is: arn:aws:sagemaker:us-east-1:159878781974:model/automl-form13-model-22-17-26-11\n"
     ]
    }
   ],
   "source": [
    "model_name = 'automl-form13-model-' + timestamp_suffix\n",
    "model = sm.create_model(Containers=best_candidate['InferenceContainers'], ModelName=model_name, ExecutionRoleArn=role)\n",
    "print('Model ARN corresponding to the best candidate is: {}'.format(model['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use batch inference through Amazon SageMaker batch transform. The same model can also be deployed to perform online inference using Amazon SageMaker hosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransformJobArn': 'arn:aws:sagemaker:us-east-1:159878781974:transform-job/automl-form13-transform-22-17-26-11',\n",
       " 'ResponseMetadata': {'RequestId': 'bfa5f648-eb27-417a-95d5-f8cbf3c8adef',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bfa5f648-eb27-417a-95d5-f8cbf3c8adef',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '112',\n",
       "   'date': 'Wed, 22 Jun 2022 19:28:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_job_name = 'automl-form13-transform-' + timestamp_suffix\n",
    "\n",
    "transform_input = {\n",
    "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': validation_data_s3_path}},\n",
    "    'ContentType': 'text/csv',\n",
    "    'CompressionType': 'None',\n",
    "    'SplitType': 'Line',\n",
    "}\n",
    "\n",
    "transform_output = {\n",
    "    'S3OutputPath': 's3://{}/{}/inference-results'.format(bucket, prefix),\n",
    "}\n",
    "\n",
    "transform_resources = {'InstanceType': 'ml.m5.4xlarge', 'InstanceCount': 1}\n",
    "\n",
    "sm.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=model_name,\n",
    "    TransformInput=transform_input,\n",
    "    TransformOutput=transform_output,\n",
    "    TransformResources=transform_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the transform job for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobStatus\n",
      "----------\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "InProgress\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "print('JobStatus')\n",
    "print('----------')\n",
    "\n",
    "describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "job_run_status = describe_response['TransformJobStatus']\n",
    "print(job_run_status)\n",
    "\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "    job_run_status = describe_response['TransformJobStatus']\n",
    "    print(job_run_status)\n",
    "    sleep(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now letâ€™s view the results of the transform job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_key = '{}/inference-results/validate_data.csv.out'.format(prefix)\n",
    "local_inference_results_path = 'inference_results.csv'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "inference_results_bucket = s3.Bucket(session.default_bucket())\n",
    "\n",
    "inference_results_bucket.download_file(s3_output_key, local_inference_results_path)\n",
    "\n",
    "data = pd.read_csv(local_inference_results_path, sep=';')\n",
    "pd.set_option('display.max_rows', 10)  # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View other candidates explored by SageMaker Autopilot\n",
    "You can view all the candidates (pipeline evaluations with different hyperparameter combinations) that were explored by SageMaker Autopilot and sort them by their final performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  automl-form13-22-17-26-11smta1hA-001-ae93f26b  0.5180100202560425\n",
      "1  automl-form13-22-17-26-11smta1hA-002-a5b1a09d  0.5171800255775452\n",
      "2  automl-form13-22-17-26-11smta1hA-003-ca4573cd  0.3507882058620453\n"
     ]
    }
   ],
   "source": [
    "candidates = sm.list_candidates_for_auto_ml_job(AutoMLJobName=auto_ml_job_name, SortBy='FinalObjectiveMetricValue')['Candidates']\n",
    "index = 0\n",
    "for candidate in candidates:\n",
    "    print(\n",
    "        str(index)\n",
    "        + '  '\n",
    "        + candidate['CandidateName']\n",
    "        + '  '\n",
    "        + str(candidate['FinalAutoMLJobObjectiveMetric']['Value'])\n",
    "    )\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Generation Notebook\n",
    "Sagemaker AutoPilot also auto-generates a Candidate Definitions notebook. This notebook can be used to interactively step through the various steps taken by the Sagemaker Autopilot to arrive at the best candidate. This notebook can also be used to override various runtime parameters like parallelism, hardware used, algorithms explored, feature extraction scripts and more.\n",
    "\n",
    "The notebook can be downloaded from the following Amazon S3 location:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-159878781974\n",
      "sagemaker/form13/output/automl-form13-22-17-26-11/sagemaker-automl-candidates/automl-form13-22-17-26-11-pr-1-4bd731bcb0f54cb39bf8f52195a924d8/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb\n",
      "SageMakerAutopilotCandidateDefinitionNotebook.ipynb\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (404) when calling the HeadObject operation: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e157766a8e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mExtraArgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    318\u001b[0m         )\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# out of this and propagate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             )\n\u001b[1;32m    359\u001b[0m             transfer_future.meta.provide_transfer_size(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 )\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
     ]
    }
   ],
   "source": [
    "x=sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation']\n",
    "\n",
    "# reformat the s3 URL into something boto3 can handle\n",
    "x=x.replace('s3://','')\n",
    "x=x.split('/')\n",
    "bucket=x[0]\n",
    "file=x[len(x)-1]\n",
    "x=x[1:]\n",
    "separator = '/'\n",
    "key = separator.join(x)\n",
    "\n",
    "print(bucket)\n",
    "print(key)\n",
    "print(file)\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, file, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Notebook\n",
    "Sagemaker Autopilot also auto-generates a Data Exploration notebook, which can be downloaded from the following Amazon S3 location:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-159878781974/sagemaker/form13/output/automl-form13-22-10-55-29/sagemaker-automl-candidates/automl-form13-22-10-55-29-pr-1-99edba60a1f94dabbe0df75e98912288/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['AutoMLJobArtifacts']['DataExplorationNotebookLocation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket)\n",
    "\n",
    "job_outputs_prefix = '{}/output/{}'.format(prefix, auto_ml_job_name)\n",
    "bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "embedding.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
